{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, we will work on the [**C-NMC_Leukemia** Dataset](https://www.kaggle.com/datasets/avk256/cnmc-leukemia/data)  \n",
    "\n",
    "##### Note: This notebook runs **only** on Kaggle, the file paths and directories are configured for Kaggle.. running the code cells won't work unless the dataset is available _locally_ and file paths correctly corresponds to system directories.  \n",
    "\n",
    "This dataset is all just pictures of blood samples of patients and non-patients, every pictures is an input sample of $450 \\times 450$ pixels, which is a managable size for `Convolutional Neural Networks`.  \n",
    "\n",
    "First, let's import necessary libraries:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-12-09T13:45:41.635449Z",
     "iopub.status.busy": "2024-12-09T13:45:41.635157Z",
     "iopub.status.idle": "2024-12-09T13:45:42.475844Z",
     "shell.execute_reply": "2024-12-09T13:45:42.475263Z",
     "shell.execute_reply.started": "2024-12-09T13:45:41.635420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /kaggle/input/cnmc-leukemia\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import absl.logging\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "path = kagglehub.dataset_download(\"avk256/cnmc-leukemia\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-09T13:45:42.477522Z",
     "iopub.status.busy": "2024-12-09T13:45:42.477277Z",
     "iopub.status.idle": "2024-12-09T13:45:42.481804Z",
     "shell.execute_reply": "2024-12-09T13:45:42.481257Z",
     "shell.execute_reply.started": "2024-12-09T13:45:42.477497Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.stderr = open('/dev/null', 'w')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "tf.debugging.set_log_device_placement(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Data Preprocessing  \n",
    "\n",
    "It's important to know the structure of the dataset: _you can view it by enabling sidebar from the `view` menu_\n",
    "\n",
    "\n",
    "The following cell splits the training data into _train_, _validation_ and _test_ sets, all being used during the **training phase** to train and test the model. While the data in the `validation_data` directory is used as the final model evaluation to determine the model's real-world performance.  \n",
    "\n",
    "The following cell consolidates all data samples into one big dataset and store it in a new directory `output_dir`, containing the 2 class labels: `all` and `hem` which corresponds to **True** and **False** respectively.\n",
    "> #### all --> Acute Lymphoblastic Leukemia, hem --> healthy.\n",
    "> #### True --> Has cancer, False --> No Cancer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T13:45:42.934958Z",
     "iopub.status.busy": "2024-12-09T13:45:42.934382Z",
     "iopub.status.idle": "2024-12-09T13:47:30.408550Z",
     "shell.execute_reply": "2024-12-09T13:47:30.407899Z",
     "shell.execute_reply.started": "2024-12-09T13:45:42.934929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images consolidated into unified directories!\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/kaggle/working/unified_dataset\"\n",
    "fold_dirs = ['fold_0/fold_0', 'fold_1/fold_1', 'fold_2/fold_2']\n",
    "\n",
    "os.makedirs(os.path.join(output_dir, \"all\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"hem\"), exist_ok=True)\n",
    "\n",
    "for fold in fold_dirs:\n",
    "    fold_path = os.path.join(path, fold)\n",
    "    \n",
    "    for label in [\"all\", \"hem\"]:\n",
    "        source_dir = os.path.join(fold_path, label)\n",
    "        target_dir = os.path.join(output_dir, label)\n",
    "        \n",
    "        for img_file in os.listdir(source_dir):\n",
    "            if img_file.endswith(\".bmp\"): \n",
    "                src_path = os.path.join(source_dir, img_file)\n",
    "                dest_path = os.path.join(target_dir, img_file)\n",
    "                \n",
    "                if not os.path.exists(dest_path):\n",
    "                    shutil.copy(src_path, dest_path)\n",
    "                    \n",
    "print(\"All images consolidated into unified directories!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's make a dataframe mapping every picture with its target output, based on what folder the picture in:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T13:47:30.410422Z",
     "iopub.status.busy": "2024-12-09T13:47:30.410088Z",
     "iopub.status.idle": "2024-12-09T13:47:30.437897Z",
     "shell.execute_reply": "2024-12-09T13:47:30.437381Z",
     "shell.execute_reply.started": "2024-12-09T13:47:30.410394Z"
    }
   },
   "outputs": [],
   "source": [
    "image_paths, labels = [], []\n",
    "\n",
    "for label_dir in [\"all\", \"hem\"]:  \n",
    "    class_dir = os.path.join(output_dir, label_dir)\n",
    "    label = label_dir \n",
    "    \n",
    "    for img_file in os.listdir(class_dir):\n",
    "        if img_file.endswith(\".bmp\"):  # Ensuring only image files are included\n",
    "            image_paths.append(os.path.join(class_dir, img_file))\n",
    "            labels.append(label)\n",
    "\n",
    "data = pd.DataFrame({\"image_path\": image_paths, \"label\": labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of converting the data into a `pd.DataFrame` is to be able to manipulate it easily and use the `train_test_split` function available in scikit-learn library.  \n",
    "We can use it to split the data twice: into train/test, then the train portion into train/validation; as following:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T13:47:30.439130Z",
     "iopub.status.busy": "2024-12-09T13:47:30.438879Z",
     "iopub.status.idle": "2024-12-09T13:47:30.484891Z",
     "shell.execute_reply": "2024-12-09T13:47:30.484254Z",
     "shell.execute_reply.started": "2024-12-09T13:47:30.439105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: label\n",
      "all    4363\n",
      "hem    2033\n",
      "Name: count, dtype: int64\n",
      "Validation data: label\n",
      "all    1454\n",
      "hem     678\n",
      "Name: count, dtype: int64\n",
      "Testing data: label\n",
      "all    1455\n",
      "hem     678\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_data, test_data = train_test_split(data, test_size=0.2, \n",
    "                                             stratify=data[\"label\"], random_state=42)\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=0.25, \n",
    "                                        stratify=train_val_data[\"label\"], random_state=42)\n",
    "\n",
    "print(\"Training data:\", train_data[\"label\"].value_counts())\n",
    "print(\"Validation data:\", val_data[\"label\"].value_counts())\n",
    "print(\"Testing data:\", test_data[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to rescale the input samples to be $224\\times224$ as many pre-trained CNN models, _if we end up using one of them_, work with this input size. We also want to _augment_ the data as it increases the diversity of the data, helping the model to generalize well.  \n",
    "We can do so using the `ImageDataGenerator` class from keras processing images library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T13:47:30.487012Z",
     "iopub.status.busy": "2024-12-09T13:47:30.486743Z",
     "iopub.status.idle": "2024-12-09T13:47:30.593816Z",
     "shell.execute_reply": "2024-12-09T13:47:30.593173Z",
     "shell.execute_reply.started": "2024-12-09T13:47:30.486986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6396 validated image filenames belonging to 2 classes.\n",
      "Found 2132 validated image filenames belonging to 2 classes.\n",
      "Found 2133 validated image filenames belonging to 2 classes.\n",
      "Class indices: {'all': 0, 'hem': 1}\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1.0/255, width_shift_range=0.05, height_shift_range=0.05, \n",
    "                             shear_range=0.05, horizontal_flip=True, fill_mode='nearest')\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(dataframe=train_data, x_col=\"image_path\", y_col=\"label\", \n",
    "                                              target_size=(224, 224), batch_size=32, class_mode=\"binary\")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(dataframe=val_data, x_col=\"image_path\", y_col=\"label\", \n",
    "                                              target_size=(224, 224), batch_size=32, class_mode=\"binary\")\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(dataframe=test_data, x_col=\"image_path\", y_col=\"label\", \n",
    "                                              target_size=(224, 224), batch_size=32, class_mode=\"binary\", shuffle=False)\n",
    "\n",
    "print(\"Class indices:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T13:47:53.820416Z",
     "iopub.status.busy": "2024-12-09T13:47:53.819995Z",
     "iopub.status.idle": "2024-12-09T13:47:53.825347Z",
     "shell.execute_reply": "2024-12-09T13:47:53.824707Z",
     "shell.execute_reply.started": "2024-12-09T13:47:53.820369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.legacy.preprocessing.image.DataFrameIterator at 0x7df052339000>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Building the model  \n",
    "\n",
    "For image classification, a **CNN** model is ideal, or we could also use a pre-trained model like **VGG16**.\n",
    "a **`Conv2D`** layer is what makes a neural network _convolutional_, it also reduces the number of trainable parameters compared to fully connected **`Dense`** layer.  \n",
    "a **`BatchNormalization`** layer is often used after a `Conv2D` layer to standardize its activation functions, which leads to faster training by keeping the distribution more stable. It also help make the model generalize well if it's overfitting.  \n",
    "a **`MaxPooling2D`** layer is essential as it downsamples the spatial dimentions, _height and width_  from the previous layers, while preserving the features. It also prevents overfitting as it reduces the number of parameters.  \n",
    "a **`Flatten`** layer is used after finishing the custom CNN layers to flatten out the **2D** dimensions into a **1D** feature vector, which is a necessary transformation before feeding the data into a `Dense` layer, _fully connected NN_.  \n",
    "a **`Dropout`** layer is used to randomly _drop out_ some of the neurons during training. It helps the model generalize better, but in most application when using `BatchNormalization` layers, it's not mandatory to dropout most of the neurons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:28:36.639244Z",
     "iopub.status.busy": "2024-12-02T22:28:36.639001Z",
     "iopub.status.idle": "2024-12-02T22:28:37.690098Z",
     "shell.execute_reply": "2024-12-02T22:28:37.689425Z",
     "shell.execute_reply.started": "2024-12-02T22:28:36.639221Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(224, 224, 3)), \n",
    "     \n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(), \n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(), \n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(), \n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(), \n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(), \n",
    "    \n",
    "    Dropout(0.45),\n",
    "    Dense(1, activation='sigmoid'), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Compiling the model  \n",
    "\n",
    "We're using the `adam` optimizer as it's a standard efficient optimizer for most ML and DL applications.  \n",
    "We're monitoring the `accuracy` of the model as it helps get a generalized idea of how good the model is, _even if we care more about **precision** in such tasks_.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T22:28:37.692025Z",
     "iopub.status.busy": "2024-12-02T22:28:37.691358Z",
     "iopub.status.idle": "2024-12-02T22:28:37.727235Z",
     "shell.execute_reply": "2024-12-02T22:28:37.726651Z",
     "shell.execute_reply.started": "2024-12-02T22:28:37.691985Z"
    }
   },
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Training the model  \n",
    "\n",
    "We're using the `EarlyStopping` technique to avoid the model overfitting the data over a large number of epochs, and then restoring its best parameter depending on lowest `validation_loss`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T20:30:35.530165Z",
     "iopub.status.busy": "2024-12-02T20:30:35.529245Z",
     "iopub.status.idle": "2024-12-02T21:24:26.718195Z",
     "shell.execute_reply": "2024-12-02T21:24:26.717520Z",
     "shell.execute_reply.started": "2024-12-02T20:30:35.530127Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_generator, validation_data=val_generator,\n",
    "                    epochs=40, callbacks=[early_stopping])\n",
    "\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T21:31:02.591110Z",
     "iopub.status.busy": "2024-12-02T21:31:02.590760Z",
     "iopub.status.idle": "2024-12-02T21:31:23.447746Z",
     "shell.execute_reply": "2024-12-02T21:31:23.447024Z",
     "shell.execute_reply.started": "2024-12-02T21:31:02.591080Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(test_generator, verbose=1)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "accuracy = classification_report(y_true, y_pred, output_dict=True)['accuracy']\n",
    "print(\"Test Accuracy: \" + str(accuracy) + '\\n' + classification_report(y_true, y_pred))\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', color='black')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='lightgreen')\n",
    "plt.axhline(y=accuracy, color='red', linestyle='--', label=\"Test Accuracy\")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T21:32:29.949713Z",
     "iopub.status.busy": "2024-12-02T21:32:29.949328Z",
     "iopub.status.idle": "2024-12-02T21:32:30.213056Z",
     "shell.execute_reply": "2024-12-02T21:32:30.212220Z",
     "shell.execute_reply.started": "2024-12-02T21:32:29.949681Z"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Hem (0)', 'All (1)'], yticklabels=['Hem (0)', 'All (1)'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Now let's save the model**:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-02T21:33:00.650818Z",
     "iopub.status.busy": "2024-12-02T21:33:00.649996Z",
     "iopub.status.idle": "2024-12-02T21:33:00.925979Z",
     "shell.execute_reply": "2024-12-02T21:33:00.925057Z",
     "shell.execute_reply.started": "2024-12-02T21:33:00.650783Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('/kaggle/working/leukemia_classifier.h5')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 319080,
     "sourceId": 643971,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 439052,
     "sourceId": 833568,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30788,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
