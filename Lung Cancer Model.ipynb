{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gk2A_F6rkJbh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import dump\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "TFggzpHEkmWr",
    "outputId": "6b95280d-af88-4fb9-fc48-f40208d559f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SMOKING</th>\n",
       "      <th>YELLOW_FINGERS</th>\n",
       "      <th>ANXIETY</th>\n",
       "      <th>PEER_PRESSURE</th>\n",
       "      <th>CHRONIC DISEASE</th>\n",
       "      <th>FATIGUE</th>\n",
       "      <th>ALLERGY</th>\n",
       "      <th>WHEEZING</th>\n",
       "      <th>ALCOHOL CONSUMING</th>\n",
       "      <th>COUGHING</th>\n",
       "      <th>SHORTNESS OF BREATH</th>\n",
       "      <th>SWALLOWING DIFFICULTY</th>\n",
       "      <th>CHEST PAIN</th>\n",
       "      <th>LUNG_CANCER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GENDER  AGE  SMOKING  YELLOW_FINGERS  ANXIETY  PEER_PRESSURE  \\\n",
       "0      M   69        1               2        2              1   \n",
       "1      M   74        2               1        1              1   \n",
       "2      F   59        1               1        1              2   \n",
       "3      M   63        2               2        2              1   \n",
       "4      F   63        1               2        1              1   \n",
       "\n",
       "   CHRONIC DISEASE  FATIGUE   ALLERGY   WHEEZING  ALCOHOL CONSUMING  COUGHING  \\\n",
       "0                1         2         1         2                  2         2   \n",
       "1                2         2         2         1                  1         1   \n",
       "2                1         2         1         2                  1         2   \n",
       "3                1         1         1         1                  2         1   \n",
       "4                1         1         1         2                  1         2   \n",
       "\n",
       "   SHORTNESS OF BREATH  SWALLOWING DIFFICULTY  CHEST PAIN LUNG_CANCER  \n",
       "0                    2                      2           2         YES  \n",
       "1                    2                      2           2         YES  \n",
       "2                    2                      1           2          NO  \n",
       "3                    1                      2           2          NO  \n",
       "4                    2                      1           1          NO  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Datasets/survey lung cancer.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "3GHaVgt0kqNz",
    "outputId": "edfa755c-c865-400d-8f99-16496779cc6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GENDER</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SMOKING</th>\n",
       "      <th>YELLOW_FINGERS</th>\n",
       "      <th>ANXIETY</th>\n",
       "      <th>PEER_PRESSURE</th>\n",
       "      <th>CHRONIC DISEASE</th>\n",
       "      <th>FATIGUE</th>\n",
       "      <th>ALLERGY</th>\n",
       "      <th>WHEEZING</th>\n",
       "      <th>ALCOHOL CONSUMING</th>\n",
       "      <th>COUGHING</th>\n",
       "      <th>SHORTNESS OF BREATH</th>\n",
       "      <th>SWALLOWING DIFFICULTY</th>\n",
       "      <th>CHEST PAIN</th>\n",
       "      <th>LUNG_CANCER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GENDER</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021306</td>\n",
       "      <td>0.036277</td>\n",
       "      <td>-0.212959</td>\n",
       "      <td>-0.152127</td>\n",
       "      <td>-0.275564</td>\n",
       "      <td>-0.204606</td>\n",
       "      <td>-0.083560</td>\n",
       "      <td>0.154251</td>\n",
       "      <td>0.141207</td>\n",
       "      <td>0.454268</td>\n",
       "      <td>0.133303</td>\n",
       "      <td>-0.064911</td>\n",
       "      <td>-0.078161</td>\n",
       "      <td>0.362958</td>\n",
       "      <td>0.067254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.021306</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.084475</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>0.053170</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>0.012614</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>0.058985</td>\n",
       "      <td>0.169950</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>-0.018104</td>\n",
       "      <td>0.089465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SMOKING</th>\n",
       "      <td>0.036277</td>\n",
       "      <td>-0.084475</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014585</td>\n",
       "      <td>0.160267</td>\n",
       "      <td>-0.042822</td>\n",
       "      <td>-0.141522</td>\n",
       "      <td>-0.029575</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>-0.129426</td>\n",
       "      <td>-0.050623</td>\n",
       "      <td>-0.129471</td>\n",
       "      <td>0.061264</td>\n",
       "      <td>0.030718</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>0.058179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YELLOW_FINGERS</th>\n",
       "      <td>-0.212959</td>\n",
       "      <td>0.005205</td>\n",
       "      <td>-0.014585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565829</td>\n",
       "      <td>0.323083</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>-0.118058</td>\n",
       "      <td>-0.144300</td>\n",
       "      <td>-0.078515</td>\n",
       "      <td>-0.289025</td>\n",
       "      <td>-0.012640</td>\n",
       "      <td>-0.105944</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>-0.104829</td>\n",
       "      <td>0.181339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANXIETY</th>\n",
       "      <td>-0.152127</td>\n",
       "      <td>0.053170</td>\n",
       "      <td>0.160267</td>\n",
       "      <td>0.565829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216841</td>\n",
       "      <td>-0.009678</td>\n",
       "      <td>-0.188538</td>\n",
       "      <td>-0.165750</td>\n",
       "      <td>-0.191807</td>\n",
       "      <td>-0.165750</td>\n",
       "      <td>-0.225644</td>\n",
       "      <td>-0.144077</td>\n",
       "      <td>0.489403</td>\n",
       "      <td>-0.113634</td>\n",
       "      <td>0.144947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PEER_PRESSURE</th>\n",
       "      <td>-0.275564</td>\n",
       "      <td>0.018685</td>\n",
       "      <td>-0.042822</td>\n",
       "      <td>0.323083</td>\n",
       "      <td>0.216841</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.048515</td>\n",
       "      <td>0.078148</td>\n",
       "      <td>-0.081800</td>\n",
       "      <td>-0.068771</td>\n",
       "      <td>-0.159973</td>\n",
       "      <td>-0.089019</td>\n",
       "      <td>-0.220175</td>\n",
       "      <td>0.366590</td>\n",
       "      <td>-0.094828</td>\n",
       "      <td>0.186388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHRONIC DISEASE</th>\n",
       "      <td>-0.204606</td>\n",
       "      <td>-0.012642</td>\n",
       "      <td>-0.141522</td>\n",
       "      <td>0.041122</td>\n",
       "      <td>-0.009678</td>\n",
       "      <td>0.048515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.110529</td>\n",
       "      <td>0.106386</td>\n",
       "      <td>-0.049967</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.175287</td>\n",
       "      <td>-0.026459</td>\n",
       "      <td>0.075176</td>\n",
       "      <td>-0.036938</td>\n",
       "      <td>0.110891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FATIGUE</th>\n",
       "      <td>-0.083560</td>\n",
       "      <td>0.012614</td>\n",
       "      <td>-0.029575</td>\n",
       "      <td>-0.118058</td>\n",
       "      <td>-0.188538</td>\n",
       "      <td>0.078148</td>\n",
       "      <td>-0.110529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.141937</td>\n",
       "      <td>-0.191377</td>\n",
       "      <td>0.146856</td>\n",
       "      <td>0.441745</td>\n",
       "      <td>-0.132790</td>\n",
       "      <td>-0.010832</td>\n",
       "      <td>0.150673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALLERGY</th>\n",
       "      <td>0.154251</td>\n",
       "      <td>0.027990</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>-0.144300</td>\n",
       "      <td>-0.165750</td>\n",
       "      <td>-0.081800</td>\n",
       "      <td>0.106386</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173867</td>\n",
       "      <td>0.344339</td>\n",
       "      <td>0.189524</td>\n",
       "      <td>-0.030056</td>\n",
       "      <td>-0.061508</td>\n",
       "      <td>0.239433</td>\n",
       "      <td>0.327766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WHEEZING</th>\n",
       "      <td>0.141207</td>\n",
       "      <td>0.055011</td>\n",
       "      <td>-0.129426</td>\n",
       "      <td>-0.078515</td>\n",
       "      <td>-0.191807</td>\n",
       "      <td>-0.068771</td>\n",
       "      <td>-0.049967</td>\n",
       "      <td>0.141937</td>\n",
       "      <td>0.173867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265659</td>\n",
       "      <td>0.374265</td>\n",
       "      <td>0.037834</td>\n",
       "      <td>0.069027</td>\n",
       "      <td>0.147640</td>\n",
       "      <td>0.249300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALCOHOL CONSUMING</th>\n",
       "      <td>0.454268</td>\n",
       "      <td>0.058985</td>\n",
       "      <td>-0.050623</td>\n",
       "      <td>-0.289025</td>\n",
       "      <td>-0.165750</td>\n",
       "      <td>-0.159973</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.191377</td>\n",
       "      <td>0.344339</td>\n",
       "      <td>0.265659</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.202720</td>\n",
       "      <td>-0.179416</td>\n",
       "      <td>-0.009294</td>\n",
       "      <td>0.331226</td>\n",
       "      <td>0.288533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUGHING</th>\n",
       "      <td>0.133303</td>\n",
       "      <td>0.169950</td>\n",
       "      <td>-0.129471</td>\n",
       "      <td>-0.012640</td>\n",
       "      <td>-0.225644</td>\n",
       "      <td>-0.089019</td>\n",
       "      <td>-0.175287</td>\n",
       "      <td>0.146856</td>\n",
       "      <td>0.189524</td>\n",
       "      <td>0.374265</td>\n",
       "      <td>0.202720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277385</td>\n",
       "      <td>-0.157586</td>\n",
       "      <td>0.083958</td>\n",
       "      <td>0.248570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHORTNESS OF BREATH</th>\n",
       "      <td>-0.064911</td>\n",
       "      <td>-0.017513</td>\n",
       "      <td>0.061264</td>\n",
       "      <td>-0.105944</td>\n",
       "      <td>-0.144077</td>\n",
       "      <td>-0.220175</td>\n",
       "      <td>-0.026459</td>\n",
       "      <td>0.441745</td>\n",
       "      <td>-0.030056</td>\n",
       "      <td>0.037834</td>\n",
       "      <td>-0.179416</td>\n",
       "      <td>0.277385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.161015</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>0.060738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWALLOWING DIFFICULTY</th>\n",
       "      <td>-0.078161</td>\n",
       "      <td>-0.001270</td>\n",
       "      <td>0.030718</td>\n",
       "      <td>0.345904</td>\n",
       "      <td>0.489403</td>\n",
       "      <td>0.366590</td>\n",
       "      <td>0.075176</td>\n",
       "      <td>-0.132790</td>\n",
       "      <td>-0.061508</td>\n",
       "      <td>0.069027</td>\n",
       "      <td>-0.009294</td>\n",
       "      <td>-0.157586</td>\n",
       "      <td>-0.161015</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069027</td>\n",
       "      <td>0.259730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHEST PAIN</th>\n",
       "      <td>0.362958</td>\n",
       "      <td>-0.018104</td>\n",
       "      <td>0.120117</td>\n",
       "      <td>-0.104829</td>\n",
       "      <td>-0.113634</td>\n",
       "      <td>-0.094828</td>\n",
       "      <td>-0.036938</td>\n",
       "      <td>-0.010832</td>\n",
       "      <td>0.239433</td>\n",
       "      <td>0.147640</td>\n",
       "      <td>0.331226</td>\n",
       "      <td>0.083958</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>0.069027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.190451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LUNG_CANCER</th>\n",
       "      <td>0.067254</td>\n",
       "      <td>0.089465</td>\n",
       "      <td>0.058179</td>\n",
       "      <td>0.181339</td>\n",
       "      <td>0.144947</td>\n",
       "      <td>0.186388</td>\n",
       "      <td>0.110891</td>\n",
       "      <td>0.150673</td>\n",
       "      <td>0.327766</td>\n",
       "      <td>0.249300</td>\n",
       "      <td>0.288533</td>\n",
       "      <td>0.248570</td>\n",
       "      <td>0.060738</td>\n",
       "      <td>0.259730</td>\n",
       "      <td>0.190451</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         GENDER       AGE   SMOKING  YELLOW_FINGERS   ANXIETY  \\\n",
       "GENDER                 1.000000  0.021306  0.036277       -0.212959 -0.152127   \n",
       "AGE                    0.021306  1.000000 -0.084475        0.005205  0.053170   \n",
       "SMOKING                0.036277 -0.084475  1.000000       -0.014585  0.160267   \n",
       "YELLOW_FINGERS        -0.212959  0.005205 -0.014585        1.000000  0.565829   \n",
       "ANXIETY               -0.152127  0.053170  0.160267        0.565829  1.000000   \n",
       "PEER_PRESSURE         -0.275564  0.018685 -0.042822        0.323083  0.216841   \n",
       "CHRONIC DISEASE       -0.204606 -0.012642 -0.141522        0.041122 -0.009678   \n",
       "FATIGUE               -0.083560  0.012614 -0.029575       -0.118058 -0.188538   \n",
       "ALLERGY                0.154251  0.027990  0.001913       -0.144300 -0.165750   \n",
       "WHEEZING               0.141207  0.055011 -0.129426       -0.078515 -0.191807   \n",
       "ALCOHOL CONSUMING      0.454268  0.058985 -0.050623       -0.289025 -0.165750   \n",
       "COUGHING               0.133303  0.169950 -0.129471       -0.012640 -0.225644   \n",
       "SHORTNESS OF BREATH   -0.064911 -0.017513  0.061264       -0.105944 -0.144077   \n",
       "SWALLOWING DIFFICULTY -0.078161 -0.001270  0.030718        0.345904  0.489403   \n",
       "CHEST PAIN             0.362958 -0.018104  0.120117       -0.104829 -0.113634   \n",
       "LUNG_CANCER            0.067254  0.089465  0.058179        0.181339  0.144947   \n",
       "\n",
       "                       PEER_PRESSURE  CHRONIC DISEASE  FATIGUE   ALLERGY   \\\n",
       "GENDER                     -0.275564        -0.204606 -0.083560  0.154251   \n",
       "AGE                         0.018685        -0.012642  0.012614  0.027990   \n",
       "SMOKING                    -0.042822        -0.141522 -0.029575  0.001913   \n",
       "YELLOW_FINGERS              0.323083         0.041122 -0.118058 -0.144300   \n",
       "ANXIETY                     0.216841        -0.009678 -0.188538 -0.165750   \n",
       "PEER_PRESSURE               1.000000         0.048515  0.078148 -0.081800   \n",
       "CHRONIC DISEASE             0.048515         1.000000 -0.110529  0.106386   \n",
       "FATIGUE                     0.078148        -0.110529  1.000000  0.003056   \n",
       "ALLERGY                    -0.081800         0.106386  0.003056  1.000000   \n",
       "WHEEZING                   -0.068771        -0.049967  0.141937  0.173867   \n",
       "ALCOHOL CONSUMING          -0.159973         0.002150 -0.191377  0.344339   \n",
       "COUGHING                   -0.089019        -0.175287  0.146856  0.189524   \n",
       "SHORTNESS OF BREATH        -0.220175        -0.026459  0.441745 -0.030056   \n",
       "SWALLOWING DIFFICULTY       0.366590         0.075176 -0.132790 -0.061508   \n",
       "CHEST PAIN                 -0.094828        -0.036938 -0.010832  0.239433   \n",
       "LUNG_CANCER                 0.186388         0.110891  0.150673  0.327766   \n",
       "\n",
       "                       WHEEZING  ALCOHOL CONSUMING  COUGHING  \\\n",
       "GENDER                 0.141207           0.454268  0.133303   \n",
       "AGE                    0.055011           0.058985  0.169950   \n",
       "SMOKING               -0.129426          -0.050623 -0.129471   \n",
       "YELLOW_FINGERS        -0.078515          -0.289025 -0.012640   \n",
       "ANXIETY               -0.191807          -0.165750 -0.225644   \n",
       "PEER_PRESSURE         -0.068771          -0.159973 -0.089019   \n",
       "CHRONIC DISEASE       -0.049967           0.002150 -0.175287   \n",
       "FATIGUE                0.141937          -0.191377  0.146856   \n",
       "ALLERGY                0.173867           0.344339  0.189524   \n",
       "WHEEZING               1.000000           0.265659  0.374265   \n",
       "ALCOHOL CONSUMING      0.265659           1.000000  0.202720   \n",
       "COUGHING               0.374265           0.202720  1.000000   \n",
       "SHORTNESS OF BREATH    0.037834          -0.179416  0.277385   \n",
       "SWALLOWING DIFFICULTY  0.069027          -0.009294 -0.157586   \n",
       "CHEST PAIN             0.147640           0.331226  0.083958   \n",
       "LUNG_CANCER            0.249300           0.288533  0.248570   \n",
       "\n",
       "                       SHORTNESS OF BREATH  SWALLOWING DIFFICULTY  CHEST PAIN  \\\n",
       "GENDER                           -0.064911              -0.078161    0.362958   \n",
       "AGE                              -0.017513              -0.001270   -0.018104   \n",
       "SMOKING                           0.061264               0.030718    0.120117   \n",
       "YELLOW_FINGERS                   -0.105944               0.345904   -0.104829   \n",
       "ANXIETY                          -0.144077               0.489403   -0.113634   \n",
       "PEER_PRESSURE                    -0.220175               0.366590   -0.094828   \n",
       "CHRONIC DISEASE                  -0.026459               0.075176   -0.036938   \n",
       "FATIGUE                           0.441745              -0.132790   -0.010832   \n",
       "ALLERGY                          -0.030056              -0.061508    0.239433   \n",
       "WHEEZING                          0.037834               0.069027    0.147640   \n",
       "ALCOHOL CONSUMING                -0.179416              -0.009294    0.331226   \n",
       "COUGHING                          0.277385              -0.157586    0.083958   \n",
       "SHORTNESS OF BREATH               1.000000              -0.161015    0.024256   \n",
       "SWALLOWING DIFFICULTY            -0.161015               1.000000    0.069027   \n",
       "CHEST PAIN                        0.024256               0.069027    1.000000   \n",
       "LUNG_CANCER                       0.060738               0.259730    0.190451   \n",
       "\n",
       "                       LUNG_CANCER  \n",
       "GENDER                    0.067254  \n",
       "AGE                       0.089465  \n",
       "SMOKING                   0.058179  \n",
       "YELLOW_FINGERS            0.181339  \n",
       "ANXIETY                   0.144947  \n",
       "PEER_PRESSURE             0.186388  \n",
       "CHRONIC DISEASE           0.110891  \n",
       "FATIGUE                   0.150673  \n",
       "ALLERGY                   0.327766  \n",
       "WHEEZING                  0.249300  \n",
       "ALCOHOL CONSUMING         0.288533  \n",
       "COUGHING                  0.248570  \n",
       "SHORTNESS OF BREATH       0.060738  \n",
       "SWALLOWING DIFFICULTY     0.259730  \n",
       "CHEST PAIN                0.190451  \n",
       "LUNG_CANCER               1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data[\"GENDER\"] = encoder.fit_transform(data[\"GENDER\"])\n",
    "data[\"LUNG_CANCER\"] = encoder.fit_transform(data[\"LUNG_CANCER\"])\n",
    "\n",
    "data.head()\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iHySrzKwkvdc"
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns=['LUNG_CANCER'])\n",
    "y = data['LUNG_CANCER']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    stratify=y, random_state=42)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OFELsAQmlcmk",
    "outputId": "06157d9e-048e-4d50-a2ba-3e2d0297948b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 33 duplicate entries among 309 entries in this dataset.\n",
      "\n",
      "After removing duplicate entries there are 276 entries in this dataset.\n"
     ]
    }
   ],
   "source": [
    "dup = data[data.duplicated()].shape[0]\n",
    "print(f\"There are {dup} duplicate entries among {data.shape[0]} entries in this dataset.\")\n",
    "\n",
    "data.drop_duplicates(keep='first',inplace=True)\n",
    "print(f\"\\nAfter removing duplicate entries there are {data.shape[0]} entries in this dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "XXSbLfadlfvU",
    "outputId": "6b3245cf-68a1-4571-cb41-152c66eb1da1"
   },
   "outputs": [],
   "source": [
    "regularization_parameter = 0.003\n",
    "\n",
    "neural_model = Sequential([\n",
    "    Input(shape=(X_train.shape[-1],)),\n",
    "    Dense(units=32, activation=\"relu\", kernel_regularizer=regularizers.l1(regularization_parameter)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(units=64, activation=\"relu\", kernel_regularizer=regularizers.l1(regularization_parameter)),\n",
    "    Dense(units=128, activation=\"relu\", kernel_regularizer=regularizers.l1(regularization_parameter)),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(units=16,activation=\"relu\", kernel_regularizer=regularizers.l1(regularization_parameter)),\n",
    "    Dense(units=1, activation=\"sigmoid\"), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,064</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m2,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │         \u001b[38;5;34m2,064\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,025</span> (50.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,025\u001b[0m (50.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,025</span> (50.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,025\u001b[0m (50.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adam = Adam(learning_rate=0.002)\n",
    "neural_model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "neural_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GsAR-R4OljNE",
    "outputId": "31abd704-d0e2-44c2-87b2-517fce4841d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 112ms/step - accuracy: 0.7922 - loss: 4.3493 - val_accuracy: 0.8710 - val_loss: 4.1308\n",
      "Epoch 2/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8446 - loss: 4.0250 - val_accuracy: 0.8710 - val_loss: 3.8350\n",
      "Epoch 3/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8670 - loss: 3.7246 - val_accuracy: 0.8710 - val_loss: 3.5680\n",
      "Epoch 4/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8670 - loss: 3.4746 - val_accuracy: 0.8710 - val_loss: 3.3333\n",
      "Epoch 5/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8482 - loss: 3.2965 - val_accuracy: 0.8710 - val_loss: 3.1099\n",
      "Epoch 6/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8696 - loss: 3.0293 - val_accuracy: 0.8710 - val_loss: 2.8935\n",
      "Epoch 7/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8790 - loss: 2.8097 - val_accuracy: 0.8710 - val_loss: 2.6892\n",
      "Epoch 8/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8639 - loss: 2.6360 - val_accuracy: 0.8710 - val_loss: 2.5002\n",
      "Epoch 9/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8628 - loss: 2.4270 - val_accuracy: 0.8710 - val_loss: 2.3188\n",
      "Epoch 10/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8644 - loss: 2.2424 - val_accuracy: 0.8710 - val_loss: 2.1409\n",
      "Epoch 11/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8758 - loss: 2.0534 - val_accuracy: 0.8710 - val_loss: 1.9720\n",
      "Epoch 12/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8826 - loss: 1.9013 - val_accuracy: 0.8710 - val_loss: 1.8138\n",
      "Epoch 13/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8654 - loss: 1.7601 - val_accuracy: 0.8710 - val_loss: 1.6674\n",
      "Epoch 14/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8868 - loss: 1.5911 - val_accuracy: 0.8710 - val_loss: 1.5321\n",
      "Epoch 15/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8670 - loss: 1.4752 - val_accuracy: 0.8710 - val_loss: 1.4075\n",
      "Epoch 16/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8571 - loss: 1.3589 - val_accuracy: 0.8710 - val_loss: 1.2925\n",
      "Epoch 17/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8743 - loss: 1.2453 - val_accuracy: 0.8710 - val_loss: 1.1877\n",
      "Epoch 18/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8654 - loss: 1.1378 - val_accuracy: 0.8710 - val_loss: 1.0914\n",
      "Epoch 19/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8675 - loss: 1.0348 - val_accuracy: 0.8710 - val_loss: 1.0060\n",
      "Epoch 20/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8831 - loss: 0.9556 - val_accuracy: 0.8710 - val_loss: 0.9299\n",
      "Epoch 21/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8738 - loss: 0.9020 - val_accuracy: 0.8710 - val_loss: 0.8653\n",
      "Epoch 22/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8764 - loss: 0.8268 - val_accuracy: 0.8710 - val_loss: 0.8099\n",
      "Epoch 23/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8639 - loss: 0.7859 - val_accuracy: 0.8710 - val_loss: 0.7623\n",
      "Epoch 24/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8696 - loss: 0.7284 - val_accuracy: 0.8710 - val_loss: 0.7191\n",
      "Epoch 25/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8560 - loss: 0.7203 - val_accuracy: 0.8710 - val_loss: 0.6800\n",
      "Epoch 26/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8623 - loss: 0.6606 - val_accuracy: 0.8710 - val_loss: 0.6468\n",
      "Epoch 27/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8732 - loss: 0.6405 - val_accuracy: 0.8710 - val_loss: 0.6171\n",
      "Epoch 28/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8847 - loss: 0.5777 - val_accuracy: 0.8710 - val_loss: 0.5908\n",
      "Epoch 29/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8857 - loss: 0.5667 - val_accuracy: 0.8710 - val_loss: 0.5673\n",
      "Epoch 30/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8852 - loss: 0.5395 - val_accuracy: 0.8710 - val_loss: 0.5461\n",
      "Epoch 31/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8691 - loss: 0.5214 - val_accuracy: 0.8710 - val_loss: 0.5271\n",
      "Epoch 32/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8738 - loss: 0.5167 - val_accuracy: 0.8710 - val_loss: 0.5100\n",
      "Epoch 33/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8826 - loss: 0.4709 - val_accuracy: 0.8548 - val_loss: 0.4949\n",
      "Epoch 34/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8798 - loss: 0.4834 - val_accuracy: 0.8710 - val_loss: 0.4821\n",
      "Epoch 35/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9258 - loss: 0.4476 - val_accuracy: 0.8710 - val_loss: 0.4700\n",
      "Epoch 36/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9061 - loss: 0.4723 - val_accuracy: 0.8710 - val_loss: 0.4579\n",
      "Epoch 37/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9134 - loss: 0.4409 - val_accuracy: 0.8710 - val_loss: 0.4469\n",
      "Epoch 38/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9344 - loss: 0.4142 - val_accuracy: 0.8710 - val_loss: 0.4368\n",
      "Epoch 39/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9375 - loss: 0.4228 - val_accuracy: 0.8710 - val_loss: 0.4290\n",
      "Epoch 40/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8963 - loss: 0.4505 - val_accuracy: 0.8871 - val_loss: 0.4218\n",
      "Epoch 41/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9280 - loss: 0.3894 - val_accuracy: 0.8871 - val_loss: 0.4136\n",
      "Epoch 42/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9261 - loss: 0.4067 - val_accuracy: 0.8871 - val_loss: 0.4061\n",
      "Epoch 43/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9387 - loss: 0.4075 - val_accuracy: 0.8871 - val_loss: 0.3993\n",
      "Epoch 44/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9449 - loss: 0.3940 - val_accuracy: 0.8871 - val_loss: 0.3934\n",
      "Epoch 45/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9128 - loss: 0.3712 - val_accuracy: 0.8871 - val_loss: 0.3869\n",
      "Epoch 46/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9371 - loss: 0.3661 - val_accuracy: 0.8871 - val_loss: 0.3804\n",
      "Epoch 47/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9073 - loss: 0.3921 - val_accuracy: 0.8871 - val_loss: 0.3745\n",
      "Epoch 48/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9398 - loss: 0.3905 - val_accuracy: 0.8871 - val_loss: 0.3695\n",
      "Epoch 49/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9323 - loss: 0.3456 - val_accuracy: 0.8871 - val_loss: 0.3645\n",
      "Epoch 50/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9241 - loss: 0.3682 - val_accuracy: 0.8871 - val_loss: 0.3596\n",
      "Epoch 51/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9316 - loss: 0.3562 - val_accuracy: 0.8871 - val_loss: 0.3560\n",
      "Epoch 52/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9372 - loss: 0.3368 - val_accuracy: 0.8871 - val_loss: 0.3522\n",
      "Epoch 53/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9486 - loss: 0.3373 - val_accuracy: 0.8871 - val_loss: 0.3493\n",
      "Epoch 54/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9056 - loss: 0.3737 - val_accuracy: 0.8871 - val_loss: 0.3449\n",
      "Epoch 55/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9428 - loss: 0.3299 - val_accuracy: 0.8871 - val_loss: 0.3422\n",
      "Epoch 56/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9613 - loss: 0.3210 - val_accuracy: 0.8871 - val_loss: 0.3396\n",
      "Epoch 57/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9419 - loss: 0.3238 - val_accuracy: 0.9032 - val_loss: 0.3366\n",
      "Epoch 58/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9384 - loss: 0.3552 - val_accuracy: 0.9032 - val_loss: 0.3342\n",
      "Epoch 59/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9477 - loss: 0.3306 - val_accuracy: 0.9032 - val_loss: 0.3313\n",
      "Epoch 60/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9425 - loss: 0.3299 - val_accuracy: 0.9032 - val_loss: 0.3271\n",
      "Epoch 61/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9289 - loss: 0.3414 - val_accuracy: 0.9032 - val_loss: 0.3241\n",
      "Epoch 62/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9513 - loss: 0.3079 - val_accuracy: 0.9032 - val_loss: 0.3213\n",
      "Epoch 63/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9497 - loss: 0.3106 - val_accuracy: 0.9032 - val_loss: 0.3193\n",
      "Epoch 64/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9450 - loss: 0.3096 - val_accuracy: 0.9032 - val_loss: 0.3166\n",
      "Epoch 65/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9298 - loss: 0.3211 - val_accuracy: 0.9032 - val_loss: 0.3141\n",
      "Epoch 66/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9467 - loss: 0.3071 - val_accuracy: 0.9194 - val_loss: 0.3124\n",
      "Epoch 67/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9387 - loss: 0.3211 - val_accuracy: 0.9194 - val_loss: 0.3106\n",
      "Epoch 68/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9430 - loss: 0.3149 - val_accuracy: 0.9355 - val_loss: 0.3075\n",
      "Epoch 69/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9473 - loss: 0.3074 - val_accuracy: 0.9355 - val_loss: 0.3056\n",
      "Epoch 70/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9476 - loss: 0.2994 - val_accuracy: 0.9355 - val_loss: 0.3045\n",
      "Epoch 71/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9382 - loss: 0.3026 - val_accuracy: 0.9355 - val_loss: 0.3031\n",
      "Epoch 72/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9599 - loss: 0.3019 - val_accuracy: 0.9355 - val_loss: 0.3007\n",
      "Epoch 73/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9539 - loss: 0.3034 - val_accuracy: 0.9355 - val_loss: 0.2990\n",
      "Epoch 74/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9566 - loss: 0.2854 - val_accuracy: 0.9355 - val_loss: 0.2973\n",
      "Epoch 75/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9398 - loss: 0.2896 - val_accuracy: 0.9516 - val_loss: 0.2953\n",
      "Epoch 76/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9567 - loss: 0.2763 - val_accuracy: 0.9677 - val_loss: 0.2938\n",
      "Epoch 77/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9472 - loss: 0.2953 - val_accuracy: 0.9516 - val_loss: 0.2925\n",
      "Epoch 78/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9541 - loss: 0.2919 - val_accuracy: 0.9516 - val_loss: 0.2914\n",
      "Epoch 79/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9478 - loss: 0.3007 - val_accuracy: 0.9516 - val_loss: 0.2894\n",
      "Epoch 80/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9524 - loss: 0.2947 - val_accuracy: 0.9677 - val_loss: 0.2879\n",
      "Epoch 81/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9536 - loss: 0.2997 - val_accuracy: 0.9677 - val_loss: 0.2878\n",
      "Epoch 82/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9420 - loss: 0.3012 - val_accuracy: 0.9355 - val_loss: 0.2863\n",
      "Epoch 83/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9588 - loss: 0.2771 - val_accuracy: 0.9355 - val_loss: 0.2866\n",
      "Epoch 84/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9441 - loss: 0.2914 - val_accuracy: 0.9355 - val_loss: 0.2868\n",
      "Epoch 85/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9520 - loss: 0.2866 - val_accuracy: 0.9355 - val_loss: 0.2851\n",
      "Epoch 86/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9714 - loss: 0.2665 - val_accuracy: 0.9355 - val_loss: 0.2834\n",
      "Epoch 87/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9508 - loss: 0.2993 - val_accuracy: 0.9516 - val_loss: 0.2808\n",
      "Epoch 88/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9368 - loss: 0.3085 - val_accuracy: 0.9516 - val_loss: 0.2786\n",
      "Epoch 89/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9504 - loss: 0.2791 - val_accuracy: 0.9516 - val_loss: 0.2778\n",
      "Epoch 90/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9236 - loss: 0.3157 - val_accuracy: 0.9516 - val_loss: 0.2777\n",
      "Epoch 91/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9453 - loss: 0.2780 - val_accuracy: 0.9516 - val_loss: 0.2774\n",
      "Epoch 92/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9682 - loss: 0.2639 - val_accuracy: 0.9516 - val_loss: 0.2777\n",
      "Epoch 93/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9518 - loss: 0.2745 - val_accuracy: 0.9355 - val_loss: 0.2781\n",
      "Epoch 94/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9561 - loss: 0.2792 - val_accuracy: 0.9194 - val_loss: 0.2803\n",
      "Epoch 95/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9462 - loss: 0.2733 - val_accuracy: 0.9355 - val_loss: 0.2784\n",
      "Epoch 96/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9666 - loss: 0.2618 - val_accuracy: 0.9355 - val_loss: 0.2760\n",
      "Epoch 97/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9377 - loss: 0.3009 - val_accuracy: 0.9516 - val_loss: 0.2757\n",
      "Epoch 98/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9508 - loss: 0.2695 - val_accuracy: 0.9516 - val_loss: 0.2762\n",
      "Epoch 99/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9425 - loss: 0.2797 - val_accuracy: 0.9516 - val_loss: 0.2762\n",
      "Epoch 100/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9630 - loss: 0.2526 - val_accuracy: 0.9516 - val_loss: 0.2754\n",
      "Epoch 101/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9461 - loss: 0.2683 - val_accuracy: 0.9516 - val_loss: 0.2751\n",
      "Epoch 102/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9645 - loss: 0.2596 - val_accuracy: 0.9355 - val_loss: 0.2762\n",
      "Epoch 103/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9420 - loss: 0.2694 - val_accuracy: 0.9355 - val_loss: 0.2747\n",
      "Epoch 104/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9673 - loss: 0.2516 - val_accuracy: 0.9355 - val_loss: 0.2725\n",
      "Epoch 105/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9515 - loss: 0.2564 - val_accuracy: 0.9516 - val_loss: 0.2686\n",
      "Epoch 106/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9514 - loss: 0.2595 - val_accuracy: 0.9516 - val_loss: 0.2671\n",
      "Epoch 107/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9378 - loss: 0.2729 - val_accuracy: 0.9516 - val_loss: 0.2682\n",
      "Epoch 108/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9419 - loss: 0.2889 - val_accuracy: 0.9516 - val_loss: 0.2683\n",
      "Epoch 109/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9604 - loss: 0.2501 - val_accuracy: 0.9516 - val_loss: 0.2656\n",
      "Epoch 110/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9435 - loss: 0.2701 - val_accuracy: 0.9516 - val_loss: 0.2625\n",
      "Epoch 111/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9619 - loss: 0.2623 - val_accuracy: 0.9677 - val_loss: 0.2609\n",
      "Epoch 112/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9588 - loss: 0.2454 - val_accuracy: 0.9677 - val_loss: 0.2603\n",
      "Epoch 113/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9384 - loss: 0.2681 - val_accuracy: 0.9677 - val_loss: 0.2604\n",
      "Epoch 114/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9461 - loss: 0.2581 - val_accuracy: 0.9677 - val_loss: 0.2614\n",
      "Epoch 115/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9594 - loss: 0.2504 - val_accuracy: 0.9677 - val_loss: 0.2614\n",
      "Epoch 116/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9524 - loss: 0.2753 - val_accuracy: 0.9516 - val_loss: 0.2607\n",
      "Epoch 117/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9519 - loss: 0.2525 - val_accuracy: 0.9516 - val_loss: 0.2609\n",
      "Epoch 118/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9426 - loss: 0.2619 - val_accuracy: 0.9516 - val_loss: 0.2612\n",
      "Epoch 119/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9442 - loss: 0.2530 - val_accuracy: 0.9677 - val_loss: 0.2609\n",
      "Epoch 120/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9356 - loss: 0.2596 - val_accuracy: 0.9677 - val_loss: 0.2605\n",
      "Epoch 121/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9519 - loss: 0.2599 - val_accuracy: 0.9677 - val_loss: 0.2592\n",
      "Epoch 122/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9467 - loss: 0.2494 - val_accuracy: 0.9516 - val_loss: 0.2589\n",
      "Epoch 123/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9592 - loss: 0.2438 - val_accuracy: 0.9516 - val_loss: 0.2581\n",
      "Epoch 124/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9524 - loss: 0.2539 - val_accuracy: 0.9516 - val_loss: 0.2578\n",
      "Epoch 125/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9425 - loss: 0.2756 - val_accuracy: 0.9516 - val_loss: 0.2582\n",
      "Epoch 126/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9391 - loss: 0.2492 - val_accuracy: 0.9516 - val_loss: 0.2586\n",
      "Epoch 127/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9440 - loss: 0.2441 - val_accuracy: 0.9516 - val_loss: 0.2578\n",
      "Epoch 128/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9409 - loss: 0.2700 - val_accuracy: 0.9516 - val_loss: 0.2570\n",
      "Epoch 129/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9408 - loss: 0.2528 - val_accuracy: 0.9516 - val_loss: 0.2572\n",
      "Epoch 130/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9582 - loss: 0.2492 - val_accuracy: 0.9516 - val_loss: 0.2563\n",
      "Epoch 131/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9503 - loss: 0.2467 - val_accuracy: 0.9516 - val_loss: 0.2580\n",
      "Epoch 132/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9435 - loss: 0.2455 - val_accuracy: 0.9516 - val_loss: 0.2586\n",
      "Epoch 133/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9481 - loss: 0.2466 - val_accuracy: 0.9516 - val_loss: 0.2587\n",
      "Epoch 134/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9535 - loss: 0.2525 - val_accuracy: 0.9516 - val_loss: 0.2569\n",
      "Epoch 135/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9629 - loss: 0.2351 - val_accuracy: 0.9516 - val_loss: 0.2550\n",
      "Epoch 136/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9467 - loss: 0.2435 - val_accuracy: 0.9516 - val_loss: 0.2528\n",
      "Epoch 137/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9414 - loss: 0.2642 - val_accuracy: 0.9516 - val_loss: 0.2526\n",
      "Epoch 138/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9529 - loss: 0.2428 - val_accuracy: 0.9516 - val_loss: 0.2526\n",
      "Epoch 139/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9534 - loss: 0.2377 - val_accuracy: 0.9516 - val_loss: 0.2515\n",
      "Epoch 140/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9241 - loss: 0.2659 - val_accuracy: 0.9516 - val_loss: 0.2504\n",
      "Epoch 141/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9441 - loss: 0.2493 - val_accuracy: 0.9516 - val_loss: 0.2493\n",
      "Epoch 142/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9529 - loss: 0.2420 - val_accuracy: 0.9516 - val_loss: 0.2478\n",
      "Epoch 143/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9619 - loss: 0.2278 - val_accuracy: 0.9516 - val_loss: 0.2499\n",
      "Epoch 144/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9293 - loss: 0.2549 - val_accuracy: 0.9516 - val_loss: 0.2521\n",
      "Epoch 145/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9613 - loss: 0.2388 - val_accuracy: 0.9516 - val_loss: 0.2511\n",
      "Epoch 146/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9467 - loss: 0.2587 - val_accuracy: 0.9516 - val_loss: 0.2511\n",
      "Epoch 147/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9651 - loss: 0.2228 - val_accuracy: 0.9516 - val_loss: 0.2510\n",
      "Epoch 148/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9340 - loss: 0.2580 - val_accuracy: 0.9516 - val_loss: 0.2521\n",
      "Epoch 149/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9550 - loss: 0.2358 - val_accuracy: 0.9355 - val_loss: 0.2531\n",
      "Epoch 150/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9351 - loss: 0.2389 - val_accuracy: 0.9516 - val_loss: 0.2492\n",
      "Epoch 151/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9482 - loss: 0.2382 - val_accuracy: 0.9516 - val_loss: 0.2476\n",
      "Epoch 152/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9394 - loss: 0.2557 - val_accuracy: 0.9516 - val_loss: 0.2486\n",
      "Epoch 153/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9403 - loss: 0.2451 - val_accuracy: 0.9516 - val_loss: 0.2459\n",
      "Epoch 154/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9529 - loss: 0.2361 - val_accuracy: 0.9516 - val_loss: 0.2466\n",
      "Epoch 155/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9467 - loss: 0.2486 - val_accuracy: 0.9516 - val_loss: 0.2444\n",
      "Epoch 156/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9556 - loss: 0.2331 - val_accuracy: 0.9516 - val_loss: 0.2432\n",
      "Epoch 157/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9577 - loss: 0.2335 - val_accuracy: 0.9516 - val_loss: 0.2436\n",
      "Epoch 158/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9650 - loss: 0.2277 - val_accuracy: 0.9516 - val_loss: 0.2451\n",
      "Epoch 159/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9503 - loss: 0.2330 - val_accuracy: 0.9516 - val_loss: 0.2476\n",
      "Epoch 160/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9503 - loss: 0.2433 - val_accuracy: 0.9516 - val_loss: 0.2501\n",
      "Epoch 161/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9640 - loss: 0.2274 - val_accuracy: 0.9355 - val_loss: 0.2502\n",
      "Epoch 162/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9514 - loss: 0.2176 - val_accuracy: 0.9516 - val_loss: 0.2503\n",
      "Epoch 163/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9546 - loss: 0.2389 - val_accuracy: 0.9516 - val_loss: 0.2449\n",
      "Epoch 164/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9513 - loss: 0.2483 - val_accuracy: 0.9516 - val_loss: 0.2402\n",
      "Epoch 165/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9513 - loss: 0.2300 - val_accuracy: 0.9516 - val_loss: 0.2409\n",
      "Epoch 166/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9467 - loss: 0.2326 - val_accuracy: 0.9516 - val_loss: 0.2423\n",
      "Epoch 167/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9572 - loss: 0.2324 - val_accuracy: 0.9516 - val_loss: 0.2420\n",
      "Epoch 168/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9500 - loss: 0.2473 - val_accuracy: 0.9516 - val_loss: 0.2406\n",
      "Epoch 169/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9597 - loss: 0.2350 - val_accuracy: 0.9516 - val_loss: 0.2412\n",
      "Epoch 170/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9245 - loss: 0.2554 - val_accuracy: 0.9516 - val_loss: 0.2451\n",
      "Epoch 171/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9511 - loss: 0.2336 - val_accuracy: 0.9516 - val_loss: 0.2488\n",
      "Epoch 172/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9721 - loss: 0.2306 - val_accuracy: 0.9516 - val_loss: 0.2535\n",
      "Epoch 173/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9614 - loss: 0.2230 - val_accuracy: 0.9516 - val_loss: 0.2549\n",
      "Epoch 174/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9452 - loss: 0.2351 - val_accuracy: 0.9516 - val_loss: 0.2525\n",
      "Epoch 175/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9525 - loss: 0.2357 - val_accuracy: 0.9516 - val_loss: 0.2551\n",
      "Epoch 176/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9519 - loss: 0.2287 - val_accuracy: 0.9516 - val_loss: 0.2557\n",
      "Epoch 177/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9373 - loss: 0.2573 - val_accuracy: 0.9516 - val_loss: 0.2553\n",
      "Epoch 178/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9468 - loss: 0.2429 - val_accuracy: 0.9516 - val_loss: 0.2556\n",
      "Epoch 179/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9456 - loss: 0.2428 - val_accuracy: 0.9516 - val_loss: 0.2528\n",
      "Epoch 180/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9587 - loss: 0.2317 - val_accuracy: 0.9516 - val_loss: 0.2528\n",
      "Epoch 181/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9567 - loss: 0.2202 - val_accuracy: 0.9516 - val_loss: 0.2558\n",
      "Epoch 182/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9398 - loss: 0.2411 - val_accuracy: 0.9516 - val_loss: 0.2586\n",
      "Epoch 183/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9414 - loss: 0.2472 - val_accuracy: 0.9355 - val_loss: 0.2614\n",
      "Epoch 184/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9366 - loss: 0.2312 - val_accuracy: 0.9355 - val_loss: 0.2623\n",
      "Epoch 185/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9587 - loss: 0.2271 - val_accuracy: 0.9516 - val_loss: 0.2578\n",
      "Epoch 186/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9189 - loss: 0.2647 - val_accuracy: 0.9516 - val_loss: 0.2542\n",
      "Epoch 187/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9571 - loss: 0.2383 - val_accuracy: 0.9516 - val_loss: 0.2510\n",
      "Epoch 188/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9405 - loss: 0.2435 - val_accuracy: 0.9516 - val_loss: 0.2520\n",
      "Epoch 189/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9379 - loss: 0.2401 - val_accuracy: 0.9516 - val_loss: 0.2536\n",
      "Epoch 190/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9535 - loss: 0.2198 - val_accuracy: 0.9516 - val_loss: 0.2560\n",
      "Epoch 191/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9671 - loss: 0.2113 - val_accuracy: 0.9516 - val_loss: 0.2538\n",
      "Epoch 192/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9478 - loss: 0.2445 - val_accuracy: 0.9516 - val_loss: 0.2537\n",
      "Epoch 193/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9488 - loss: 0.2430 - val_accuracy: 0.9516 - val_loss: 0.2554\n",
      "Epoch 194/200\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9598 - loss: 0.2190 - val_accuracy: 0.9516 - val_loss: 0.2535\n",
      "Best validation accuracy: 96.77%\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "history = neural_model.fit(X_train, y_train, epochs=200, verbose=1, batch_size=64,\n",
    "                           validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "print(f\"Best validation accuracy: {max(history.history['val_accuracy']) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jizfstUrvPfa",
    "outputId": "14bb52ad-61b6-4964-bfb1-63f0db91e5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "Test Accuracy: 0.9516129032258065 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.82         8\n",
      "           1       0.98      0.96      0.97        54\n",
      "\n",
      "    accuracy                           0.95        62\n",
      "   macro avg       0.88      0.92      0.90        62\n",
      "weighted avg       0.95      0.95      0.95        62\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = neural_model.predict(X_test)\n",
    "pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = classification_report(y_test, pred, output_dict=True)['accuracy']\n",
    "print(\"Test Accuracy:\", accuracy, '\\n', classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9o9-lAnymXx",
    "outputId": "c0b3e994-85dd-4741-a02d-418f50281d91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Models/lung_metadata.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_model.save('Models/lung_classifier.h5')\n",
    "\n",
    "metadata = {\"std_scaler\": scaler}\n",
    "dump(metadata, \"Models/lung_metadata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's save a few samples for testing later:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data.groupby('LUNG_CANCER', group_keys=False).apply(lambda x: x.sample(min(len(x), 5)))\n",
    "subset = subset.drop(columns=['LUNG_CANCER'])\n",
    "\n",
    "for i in range(subset.shape[0]):\n",
    "    subset.iloc[i].to_csv(f\"Test Samples/lung/sample_{i+1}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's test the model on the saved samples:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "Sample 1 prediction: [[0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Sample 2 prediction: [[0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Sample 3 prediction: [[0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Sample 4 prediction: [[0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Sample 5 prediction: [[0]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Sample 6 prediction: [[1]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Sample 7 prediction: [[1]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Sample 8 prediction: [[1]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Sample 9 prediction: [[1]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Sample 10 prediction: [[1]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(subset.shape[0]):\n",
    "    sample = pd.read_csv(f\"Test Samples/lung/sample_{i+1}.csv\")\n",
    "    sample = scaler.transform(sample.to_numpy().reshape(1, -1))\n",
    "    pred = neural_model.predict(sample)\n",
    "    pred = (pred > 0.5).astype(int)\n",
    "    print(f\"Sample {i+1} prediction:\", pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems to be working correctly, than means our work here is finally done!  \n",
    "\n",
    "### End of Notebook"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Mine and mine alone!!",
   "language": "python",
   "name": "local_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
